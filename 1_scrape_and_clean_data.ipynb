{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem\n",
    "\n",
    "In recent years there have been significant reductions in local authority funding,\n",
    "and budgets have been reduced accordingly.\n",
    "Cutting spending on preventative measures like early help in schools, quality of life\n",
    "improvements for elderly and disabled residents, could lead to having to spend\n",
    "more later to fix more severe problems as a result.\n",
    "Using a sample spending dataset from a local authority, design a model to test this\n",
    "theory. (Look for ‘Spend over £500’ open data).\n",
    "Remember:\n",
    "* What question are you answering?\n",
    "* If you had an answer, what would it look like?\n",
    "* What data would be relevant?\n",
    "* What are the principal components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from natsort import natsorted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data\n",
    "\n",
    "- Bristol Council did provide some over £500 spending data as a single file download.\n",
    "\n",
    "- However, they also provided more data (going back further) as monthly `.csv` files.\n",
    "\n",
    "- So task 1 was to download and combine all this data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a web scraper to download and combine .csv files.\n",
    "\n",
    "- I used the `BeautifulSoup` webscraper to do this.\n",
    "\n",
    "- The council's website was returning a 403 error when it was being scraped. This was due to it being set up to reject web scrape requests. So we need to set up a fake agent to make the website think the requests are coming from a real user rather than a scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy header to make the website think a real user is requesting the data\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'}\n",
    "\n",
    "# URL of the webpage containing CSV files\n",
    "url = 'https://www.bristol.gov.uk/council-and-mayor/council-spending-and-performance/spending-over-500'\n",
    "\n",
    "# Download the webpage content\n",
    "response = requests.get(url, headers=HEADERS)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# By inspecting the webpage in chrome I found that all the links to the\n",
    "# csv files had a 'type' so we can tell the scraper to look for this type.\n",
    "# Find all links with a type attribute of 'text/csv' and extract their URLs\n",
    "links = []\n",
    "for link in soup.find_all('a', type='text/csv'):\n",
    "    links.append(urljoin(url, link.get('href')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all CSV files and save them in the csv_folder\n",
    "\n",
    "# set a variable that we can loop through in order to name all the .csv files\n",
    "i=0\n",
    "\n",
    "# iterate over the list of links and download each csv file\n",
    "for link in links:\n",
    "    try:\n",
    "        file_response = requests.get(link, headers=HEADERS)\n",
    "    \n",
    "        # save the file to the csv_files folder\n",
    "        with open(f\"csv_files original scrape/{str(i)}.csv\", \"wb\") as f:\n",
    "            f.write(file_response.content)\n",
    "            i += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading CSV file from {link}: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean the csv files\n",
    "\n",
    "- The files don't share the same formatting (unfotunately!) so we need to clean them up before combining them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the directory containing the CSV files\n",
    "DIRECTORY = \"csv_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/samuelspeller/Documents/GitHub/Bristol-council-spending-analysis\n"
     ]
    }
   ],
   "source": [
    "# rename the csv files to show month and date.\n",
    "\n",
    "# the files were downloaded in month order so we can set the start month and year\n",
    "# and then decrease each in descending order.\n",
    "\n",
    "start_month = 2\n",
    "start_year = 2023\n",
    "\n",
    "# sort the list of files in ascending order (natsort can sort string numbers properly)\n",
    "wd = os.getcwd()\n",
    "print(wd)\n",
    "\n",
    "file_list = os.listdir(DIRECTORY)\n",
    "file_list = natsorted(file_list)\n",
    "\n",
    "# iterate over the csv files\n",
    "for filename in file_list:\n",
    "    os.rename(DIRECTORY + filename, DIRECTORY + f'{start_year}-{start_month}.csv')\n",
    "    start_month = start_month - 1\n",
    "\n",
    "    # A loop to reduce the year when we get to January and then reset the month back to December\n",
    "    if start_month == 0:\n",
    "        start_month = 12\n",
    "        start_year = start_year - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the csv files\n",
    "for filename in os.listdir(DIRECTORY):\n",
    "    if filename.endswith('.csv'):\n",
    "        # read the CSV file into a pandas DataFrame \n",
    "        df = pd.read_csv(os.path.join(DIRECTORY, filename), encoding='latin', on_bad_lines='warn')\n",
    "\n",
    "        # dictionary of column names to rename\n",
    "        rename_columns = {\n",
    "            'Name': 'Supplier', \n",
    "            'Description Line 1': 'Description 1',\n",
    "            'Description Line 2': 'Description 2',\n",
    "            'Description Line 3': 'Description 3'\n",
    "        }\n",
    "        # rename columns\n",
    "        for key, value in rename_columns.items():\n",
    "            if key in df.columns:\n",
    "                df.rename(columns={key:value}, inplace = True)\n",
    "\n",
    "        # Some of the data doesn't contain transaction dates so lets at least add\n",
    "        # the month and year from the file name.\n",
    "        \n",
    "        if 'Pay Date' not in df.columns:\n",
    "            date = os.path.splitext(filename)\n",
    "            df.insert(2, 'Pay Date', date[0])\n",
    "\n",
    "        # Some of the files have transaction numbers and some don't.\n",
    "        # Lets drop all of the transaction/ref columns.\n",
    "        # Create a list of column names to delete\n",
    "        del_columns = ['Body', 'Body Name', 'Transaction Number', 'Ref', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 0']\n",
    "        for name in del_columns:\n",
    "            if name in df.columns:\n",
    "                df = df.drop(columns=[name])\n",
    "        \n",
    "        # add description 3 column if it's missing\n",
    "        #if 'Description 3' not in df.columns:\n",
    "        #    df.insert(5, 'Description 3', ' ' )\n",
    "\n",
    "        # look for all the files which don't have the correct amount of columns\n",
    "        if len(df.columns) != 6:\n",
    "            print(filename)\n",
    "        \n",
    "        # save the modified DataFrame back to the CSV file\n",
    "        df.to_csv(os.path.join(DIRECTORY, filename), index=False)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I want to check that all the csv files have the same column names\n",
    "\n",
    "- I want to do this as a final check before combining all the `.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all columns match!\n"
     ]
    }
   ],
   "source": [
    "# Define path to the directory containing CSV files\n",
    "DIRECTORY = 'csv_files/'\n",
    "csv_files = sorted(os.listdir(DIRECTORY))\n",
    "\n",
    "# Read the first CSV file in the list to get the column names\n",
    "first_file = pd.read_csv(os.path.join(DIRECTORY, csv_files[0]))\n",
    "column_names = first_file.columns.tolist()\n",
    "\n",
    "# Loop through the rest of the CSV files and check if they have the same columns\n",
    "for file in csv_files[1:]:\n",
    "    next_file = pd.read_csv(os.path.join(DIRECTORY, file))\n",
    "    if next_file.columns.tolist() != column_names:\n",
    "        print(f'Column names in {format(file)} do not match')\n",
    "        exit()\n",
    "\n",
    "print('all columns match!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the csv files\n",
    "Now we can combine all the csv files so we have one large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the directory containing the CSV files\n",
    "DIRECTORY = \"csv_files/\"\n",
    "\n",
    "# create an empty list to store the dataframes\n",
    "dataframes = []\n",
    "\n",
    "# loop through the CSV files in the directory and append their dataframes to the list\n",
    "for filename in os.listdir(DIRECTORY):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        filepath = os.path.join(DIRECTORY, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        dataframes.append(df)\n",
    "\n",
    "# concatenate the dataframes in the list into a single dataframe\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# write the combined dataframe to a new CSV file\n",
    "combined_df.to_csv('bristol_spending_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are lots of weird characters present in the dataframe \n",
    "- This is probably due to encoding issues.\n",
    "\n",
    "- Lets remove these characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# characters we want to remove\n",
    "chars_to_remove = ['Â', 'Ã']\n",
    "\n",
    "# define a function to replace letters with an empty string\n",
    "def clean_text(x, chars_to_remove):\n",
    "    if isinstance(x, str):\n",
    "        for char in chars_to_remove:\n",
    "            x = x.replace(char, '')\n",
    "        return x\n",
    "    else: \n",
    "        return x\n",
    "\n",
    "# read csv\n",
    "df = pd.read_csv('bristol_spending_data.csv')\n",
    "\n",
    "# apply the function to each element in the DataFrame using applymap()\n",
    "df_cleaned = df.applymap(lambda x: clean_text(x, chars_to_remove))\n",
    "df_cleaned.to_csv('bristol_spending_data_final.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and combine complete\n",
    "We now have a cleaned set of data. \n",
    "\n",
    "- All the columns match.\n",
    "- Transactions with no timestamp have been given one based on the month they occured in (not the best but better than nothing).\n",
    "- We are now ready to investigate this data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
